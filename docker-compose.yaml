version: "3.8"

services:
  vllm-cpu:
    build:
      context: .
      dockerfile: docker/Dockerfile.cpu
      target: vllm-openai
      args:
        - GIT_REPO_CHECK=0
    image: vllm-cpu-env
    container_name: vllm-cpu-server
    privileged: true
    shm_size: 4g
    ports:
      - "8000:8000"
    environment:
      # Set these environment variables according to your system
      - VLLM_CPU_KVCACHE_SPACE=4
      - VLLM_CPU_OMP_THREADS_BIND=0-7
    command: >
      --model=meta-llama/Llama-3.2-1B-Instruct
      --dtype=bfloat16
      --host=0.0.0.0
      --port=8000
    # Optional: Add volumes for model caching
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    restart: unless-stopped
    # Optional: Resource limits
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G
